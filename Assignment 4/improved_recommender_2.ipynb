{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0eeec03244d7422b81151f287d9a0ca9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_272d63610aeb4386a81c7cf450a83817",
              "IPY_MODEL_cd881c9e2a9544f49cf46d4888bc2696",
              "IPY_MODEL_004253ae5f3e4b4b83976d0ed67192e3"
            ],
            "layout": "IPY_MODEL_17ed6c8f491b4c2b9e02dfbf3e932db2"
          }
        },
        "272d63610aeb4386a81c7cf450a83817": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1815683d0c53447db09224019dd20960",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3cf48b1c35924d69abedea3ee227494d",
            "value": "Computingâ€‡transitionâ€‡probabilities:â€‡100%"
          }
        },
        "cd881c9e2a9544f49cf46d4888bc2696": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7084370258c545dc9366578c8ea40240",
            "max": 884706,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_80fcef76d84149b2b268dbd6e3ef9043",
            "value": 884706
          }
        },
        "004253ae5f3e4b4b83976d0ed67192e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90befb433bf44b688eedaedbf3f6a4b2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6b6d7a798c69412b83bd6c069415f7cd",
            "value": "â€‡884706/884706â€‡[15:29&lt;00:00,â€‡1090.03it/s]"
          }
        },
        "17ed6c8f491b4c2b9e02dfbf3e932db2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1815683d0c53447db09224019dd20960": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cf48b1c35924d69abedea3ee227494d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7084370258c545dc9366578c8ea40240": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80fcef76d84149b2b268dbd6e3ef9043": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "90befb433bf44b688eedaedbf3f6a4b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b6d7a798c69412b83bd6c069415f7cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Uninstall gensim, numpy, and scipy\n",
        "! pip uninstall -y gensim numpy scipy\n",
        "\n",
        "# Reinstall the necessary packages, forcing no cache to ensure fresh install/build\n",
        "# Ensure numpy and scipy are installed before gensim/node2vec\n",
        "! pip install --no-cache-dir numpy scipy matplotlib seaborn networkx tqdm torch torch_geometric scikit-learn\n",
        "! pip install --no-cache-dir gensim node2vec\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict, Counter\n",
        "import networkx as nx\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import specific classes from libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric\n",
        "from torch_geometric.data import HeteroData\n",
        "from torch_geometric.nn import HeteroConv, SAGEConv\n",
        "from torch_geometric.utils import negative_sampling\n",
        "from torch.optim import Adam\n",
        "from node2vec import Node2Vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enlq-W7xYy5B",
        "outputId": "f921b9ed-6b2d-4af9-ac57-52a0df598da9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: gensim 4.3.3\n",
            "Uninstalling gensim-4.3.3:\n",
            "  Successfully uninstalled gensim-4.3.3\n",
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Found existing installation: scipy 1.13.1\n",
            "Uninstalling scipy-1.13.1:\n",
            "  Successfully uninstalled scipy-1.13.1\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy\n",
            "  Downloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.3)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.7.0+cu118)\n",
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.86)\n",
            "Requirement already satisfied: triton==3.3.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.3.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch) (70.2.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.12.6)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (7.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.4.26)\n",
            "Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m243.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m307.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "node2vec 0.5.0 requires gensim<5.0.0,>=4.3.0, which is not installed.\n",
            "node2vec 0.5.0 requires numpy<2.0.0,>=1.24.0, but you have numpy 2.2.6 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\n",
            "pytensor 2.30.3 requires filelock>=3.15, but you have filelock 3.13.1 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.6 scipy-1.15.3\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: node2vec in /usr/local/lib/python3.11/dist-packages (0.5.0)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: joblib<2.0.0,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from node2vec) (1.5.1)\n",
            "Requirement already satisfied: networkx<4.0.0,>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from node2vec) (3.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from node2vec) (4.67.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m312.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m279.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m328.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.6\n",
            "    Uninstalling numpy-2.2.6:\n",
            "      Successfully uninstalled numpy-2.2.6\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "pytensor 2.30.3 requires filelock>=3.15, but you have filelock 3.13.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "CONFIG = {\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    'node2vec_dims': 64,        # Reduced from 128 for faster training\n",
        "    'node2vec_walk_length': 20, # Reduced from 80 for large graphs\n",
        "    'node2vec_num_walks': 5,    # Reduced from 10 to speed up\n",
        "    'node2vec_workers': 8,      # Increased workers for parallel processing\n",
        "    'gnn_hidden_dims': 256,\n",
        "    'gnn_out_dims': 64,\n",
        "    'learning_rate': 0.001,\n",
        "    'batch_size': 1024,\n",
        "    'epochs': 100,\n",
        "    'early_stopping': 10,\n",
        "    # NEW: Recommendation diversity settings\n",
        "    'diversity_weight': 0.3,    # Weight for diversity in recommendations\n",
        "    'score_temperature': 2.0,  # Temperature for score calibration\n",
        "    'min_score_diff': 0.05     # Minimum score difference for diverse recommendations\n",
        "}\n",
        "\n",
        "print(f\"ğŸ”§ Device: {CONFIG['device']}\")\n",
        "print(f\"ğŸ”§ PyTorch: {torch.__version__}\")\n",
        "print(f\"ğŸ”§ PyTorch Geometric: {torch_geometric.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SEK2jhUWV_y",
        "outputId": "5e5f8fee-6abe-4bf9-d351-90a8bb8c8b54"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Device: cuda\n",
            "ğŸ”§ PyTorch: 2.7.0+cu118\n",
            "ğŸ”§ PyTorch Geometric: 2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader:\n",
        "    \"\"\"Handles loading and preprocessing of Steam graph data\"\"\"\n",
        "\n",
        "    def __init__(self, data_path=\"outputs/\"):\n",
        "        self.data_path = data_path\n",
        "        self.users = {}\n",
        "        self.apps = {}\n",
        "        self.user_friendships = []\n",
        "        self.user_app_reviews = []\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load all graph data from files\"\"\"\n",
        "        print(\"\\nğŸ“‚ Loading Steam graph data...\")\n",
        "\n",
        "        # Load nodes\n",
        "        print(\"  Loading nodes...\")\n",
        "        with open(f\"{self.data_path}final_recommender_graph_nodes.json\", 'r', encoding='utf-8') as f:\n",
        "            all_nodes = json.load(f)\n",
        "\n",
        "        # Separate users and apps\n",
        "        for node_id, node_data in all_nodes.items():\n",
        "            if node_data['type'] == 'User':\n",
        "                self.users[node_id] = node_data\n",
        "            else:\n",
        "                self.apps[node_id] = node_data\n",
        "\n",
        "        print(f\"    Users: {len(self.users):,}\")\n",
        "        print(f\"    Apps: {len(self.apps):,}\")\n",
        "\n",
        "        # Load edges\n",
        "        print(\"  Loading edges...\")\n",
        "\n",
        "        # Debug: Show first few lines to understand format\n",
        "        print(\"  Analyzing edge file format...\")\n",
        "        with open(f\"{self.data_path}final_recommender_graph_edges.txt\", 'r', encoding='utf-8') as f:\n",
        "            header = f.readline().strip()\n",
        "            print(f\"    Header: {header}\")\n",
        "\n",
        "            # Show first 3 data lines\n",
        "            for i in range(3):\n",
        "                line = f.readline().strip()\n",
        "                if line:\n",
        "                    parts = line.split('\\t')\n",
        "                    print(f\"    Sample line {i+1}: {len(parts)} columns -> {parts}\")\n",
        "\n",
        "        print(\"  Processing all edges...\")\n",
        "        with open(f\"{self.data_path}final_recommender_graph_edges.txt\", 'r', encoding='utf-8') as f:\n",
        "            next(f)  # Skip header\n",
        "            for line in tqdm(f, desc=\"    Processing edges\"):\n",
        "                parts = line.strip().split('\\t')\n",
        "                if len(parts) >= 6:  # Need at least: source_id, target_id, source_type, target_type, edge_type, weight\n",
        "                    try:\n",
        "                        # Correct format: source_id, target_id, source_type, target_type, edge_type, weight, sentiment_score, source_name, target_name\n",
        "                        source = parts[0]       # Source ID\n",
        "                        target = parts[1]       # Target ID\n",
        "                        source_type = parts[2]  # Source type\n",
        "                        target_type = parts[3]  # Target type\n",
        "                        edge_type = parts[4]    # Edge type (friendship/review)\n",
        "                        weight = float(parts[5]) # Weight\n",
        "\n",
        "                        if edge_type == 'friendship' and source_type == 'User' and target_type == 'User':\n",
        "                            # Friendship edge between users\n",
        "                            if source != target:  # Avoid self-loops\n",
        "                                self.user_friendships.append((source, target, weight))\n",
        "\n",
        "                        elif edge_type == 'review' and source_type == 'User' and target_type == 'App':\n",
        "                            # Review edge from user to app\n",
        "                            sentiment = 0.0\n",
        "                            if len(parts) > 6:  # Sentiment score in column 6\n",
        "                                try:\n",
        "                                    sentiment = float(parts[6])\n",
        "                                except (ValueError, IndexError):\n",
        "                                    sentiment = 0.0\n",
        "\n",
        "                            self.user_app_reviews.append((source, target, weight, sentiment))\n",
        "\n",
        "                    except (ValueError, IndexError) as e:\n",
        "                        # Only show first few warnings to avoid spam\n",
        "                        if len(self.user_friendships) + len(self.user_app_reviews) < 3:\n",
        "                            print(f\"    Warning: Skipping malformed line: {e}\")\n",
        "                        continue\n",
        "\n",
        "        print(f\"    Friendships: {len(self.user_friendships):,}\")\n",
        "        print(f\"    Reviews: {len(self.user_app_reviews):,}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def extract_largest_connected_component(self):\n",
        "        \"\"\"Extract largest connected component and prepare final dataset structure\"\"\"\n",
        "        print(\"\\nğŸ”— Preparing final dataset structure...\")\n",
        "\n",
        "        # Step 1: Get users with complete profiles\n",
        "        valid_user_ids = set(self.users.keys())\n",
        "        print(f\"  Users with complete profiles: {len(valid_user_ids):,}\")\n",
        "\n",
        "        # Step 2: Filter friendships to only include users with complete profiles\n",
        "        print(\"  Filtering friendships to profiled users...\")\n",
        "        filtered_friendships = []\n",
        "        for u, v, w in self.user_friendships:\n",
        "            if u in valid_user_ids and v in valid_user_ids:\n",
        "                filtered_friendships.append((u, v, w))\n",
        "\n",
        "        print(f\"    Friendships: {len(filtered_friendships):,} (was {len(self.user_friendships):,})\")\n",
        "\n",
        "        # Step 3: Build friendship graph and find largest connected component\n",
        "        print(\"  Building friendship network...\")\n",
        "        G = nx.Graph()\n",
        "        G.add_nodes_from(valid_user_ids)\n",
        "\n",
        "        friendship_edges = [(u, v) for u, v, w in filtered_friendships]\n",
        "        G.add_edges_from(friendship_edges)\n",
        "\n",
        "        print(f\"    Network: {G.number_of_nodes():,} users, {G.number_of_edges():,} friendships\")\n",
        "\n",
        "        # Find largest connected component\n",
        "        print(\"  Finding largest connected component...\")\n",
        "        connected_components = list(nx.connected_components(G))\n",
        "\n",
        "        if len(connected_components) == 0:\n",
        "            raise ValueError(\"No connected components found in friendship network!\")\n",
        "\n",
        "        largest_cc = max(connected_components, key=len)\n",
        "        G_lcc = G.subgraph(largest_cc).copy()\n",
        "\n",
        "        print(f\"    Total connected components: {len(connected_components)}\")\n",
        "        print(f\"    Largest CC: {G_lcc.number_of_nodes():,} users ({len(largest_cc)/len(valid_user_ids)*100:.1f}%)\")\n",
        "        print(f\"    LCC friendships: {G_lcc.number_of_edges():,}\")\n",
        "\n",
        "        # Step 4: Filter all data to LCC users only\n",
        "        lcc_users = set(largest_cc)\n",
        "        print(f\"  Filtering all data to LCC users...\")\n",
        "\n",
        "        # Filter users (keep only LCC users with complete profiles)\n",
        "        original_user_count = len(self.users)\n",
        "        self.users = {uid: data for uid, data in self.users.items() if uid in lcc_users}\n",
        "        print(f\"    Users: {len(self.users):,} (was {original_user_count:,})\")\n",
        "\n",
        "        # Filter friendships (User-User edges, NO WEIGHTS for final GNN)\n",
        "        self.user_friendships = [(u, v, w) for u, v, w in filtered_friendships\n",
        "                                if u in lcc_users and v in lcc_users]\n",
        "        print(f\"    User-User edges: {len(self.user_friendships):,}\")\n",
        "\n",
        "        # CRITICAL: Verify final User-User graph is fully connected\n",
        "        print(\"  Verifying final User-User graph connectivity...\")\n",
        "        final_G = nx.Graph()\n",
        "        final_G.add_nodes_from(lcc_users)\n",
        "        final_edges = [(u, v) for u, v, w in self.user_friendships]\n",
        "        final_G.add_edges_from(final_edges)\n",
        "\n",
        "        # Check connectivity\n",
        "        final_components = list(nx.connected_components(final_G))\n",
        "        is_connected = nx.is_connected(final_G)\n",
        "\n",
        "        print(f\"    Final User-User graph: {final_G.number_of_nodes():,} nodes, {final_G.number_of_edges():,} edges\")\n",
        "        print(f\"    Is fully connected: {is_connected}\")\n",
        "        print(f\"    Number of components: {len(final_components)}\")\n",
        "\n",
        "        if not is_connected:\n",
        "            print(f\"    âš ï¸  WARNING: Final User-User graph is NOT fully connected!\")\n",
        "            print(f\"    Component sizes: {[len(comp) for comp in sorted(final_components, key=len, reverse=True)[:5]]}\")\n",
        "\n",
        "            # This should not happen if LCC was extracted correctly, but let's handle it\n",
        "            largest_final_cc = max(final_components, key=len)\n",
        "            print(f\"    Using largest component of final graph: {len(largest_final_cc):,} users\")\n",
        "\n",
        "            # Re-filter to ensure connectivity\n",
        "            lcc_users = set(largest_final_cc)\n",
        "            self.users = {uid: data for uid, data in self.users.items() if uid in lcc_users}\n",
        "            self.user_friendships = [(u, v, w) for u, v, w in self.user_friendships\n",
        "                                    if u in lcc_users and v in lcc_users]\n",
        "\n",
        "            # Verify again\n",
        "            final_G = nx.Graph()\n",
        "            final_G.add_nodes_from(lcc_users)\n",
        "            final_edges = [(u, v) for u, v, w in self.user_friendships]\n",
        "            final_G.add_edges_from(final_edges)\n",
        "\n",
        "            print(f\"    After re-filtering: {final_G.number_of_nodes():,} nodes, {final_G.number_of_edges():,} edges\")\n",
        "            print(f\"    Is fully connected: {nx.is_connected(final_G)}\")\n",
        "\n",
        "            # Update G_lcc to the final connected graph\n",
        "            G_lcc = final_G\n",
        "\n",
        "        # Filter reviews (User-App edges with playtime weights + sentiment)\n",
        "        original_review_count = len(self.user_app_reviews)\n",
        "        self.user_app_reviews = [(u, a, w, s) for u, a, w, s in self.user_app_reviews\n",
        "                                if u in lcc_users]\n",
        "        print(f\"    User-App edges: {len(self.user_app_reviews):,} (was {original_review_count:,})\")\n",
        "\n",
        "        # IMPORTANT: Filter apps to only those reviewed by LCC users\n",
        "        print(\"  Filtering apps to only those reviewed by LCC users...\")\n",
        "        reviewed_apps = set()\n",
        "        for u, a, w, s in self.user_app_reviews:\n",
        "            reviewed_apps.add(a)\n",
        "\n",
        "        original_app_count = len(self.apps)\n",
        "        self.apps = {aid: data for aid, data in self.apps.items() if aid in reviewed_apps}\n",
        "        print(f\"    Apps: {len(self.apps):,} (was {original_app_count:,}) - only apps reviewed by LCC users\")\n",
        "\n",
        "        # Step 5: Summary of final dataset\n",
        "        print(f\"\\nğŸ“Š Final Dataset Structure:\")\n",
        "        print(f\"    ğŸ”— User-User Graph (LCC): {len(self.users):,} users, {len(self.user_friendships):,} friendships\")\n",
        "        print(f\"    ğŸ® User-App Reviews: {len(self.user_app_reviews):,} reviews (playtime weighted)\")\n",
        "        print(f\"    ğŸ¯ Apps: {len(self.apps):,} apps (only those reviewed by LCC users)\")\n",
        "        print(f\"    ğŸ‘¥ User Attributes: loccountrycode only\")\n",
        "        print(f\"    ğŸ¯ App Attributes: name, category, app_type, original_id\")\n",
        "        print(f\"    âš–ï¸  Edge Attributes: User-App (playtime + sentiment), User-User (NO WEIGHTS in final GNN)\")\n",
        "        print(f\"    âœ… User-User graph is fully connected: {nx.is_connected(G_lcc)}\")\n",
        "\n",
        "        return G_lcc\n",
        "\n",
        "class Node2VecEmbedder:\n",
        "    \"\"\"Generates Node2Vec embeddings for user nodes with attribute-aware weights\"\"\"\n",
        "\n",
        "    def __init__(self, config, data_loader=None):\n",
        "        self.config = config\n",
        "        self.data_loader = data_loader\n",
        "        self.model = None\n",
        "        self.embeddings = None\n",
        "\n",
        "    def fit(self, friendship_graph, user_app_reviews=None, user_to_idx=None):\n",
        "        \"\"\"Train Node2Vec on the friendship network with enhanced edge weights\"\"\"\n",
        "        print(f\"\\nğŸ¯ Training Node2Vec embeddings with attribute-aware weights...\")\n",
        "        print(f\"  Dimensions: {self.config['node2vec_dims']}\")\n",
        "        print(f\"  Walk length: {self.config['node2vec_walk_length']}\")\n",
        "        print(f\"  Walks per node: {self.config['node2vec_num_walks']}\")\n",
        "\n",
        "        # Enhance the friendship graph with user activity weights\n",
        "        if user_app_reviews and user_to_idx:\n",
        "            print(\"  ğŸ”— Enhancing friendship edges with user activity + country similarity...\")\n",
        "            self.enhanced_graph = self._enhance_friendship_weights(\n",
        "                friendship_graph, user_app_reviews, user_to_idx\n",
        "            )\n",
        "        else:\n",
        "            self.enhanced_graph = friendship_graph\n",
        "\n",
        "        print(f\"  ğŸ“Š Final graph: {self.enhanced_graph.number_of_nodes():,} nodes, {self.enhanced_graph.number_of_edges():,} edges\")\n",
        "\n",
        "        # Create Node2Vec model\n",
        "        self.model = Node2Vec(\n",
        "            self.enhanced_graph,\n",
        "            dimensions=self.config['node2vec_dims'],\n",
        "            walk_length=self.config['node2vec_walk_length'],\n",
        "            num_walks=self.config['node2vec_num_walks'],\n",
        "            workers=self.config['node2vec_workers'],\n",
        "            p=1.0,  # Return parameter\n",
        "            q=1.0   # In-out parameter\n",
        "        )\n",
        "\n",
        "        # Train embeddings\n",
        "        print(\"  ğŸ”„ Training Node2Vec model...\")\n",
        "        model = self.model.fit(\n",
        "            window=10,\n",
        "            min_count=1,\n",
        "            batch_words=4,\n",
        "            epochs=10\n",
        "        )\n",
        "\n",
        "        # Extract embeddings with progress bar\n",
        "        user_ids = list(self.enhanced_graph.nodes())\n",
        "        self.embeddings = {}\n",
        "\n",
        "        print(\"  ğŸ“¦ Extracting embeddings...\")\n",
        "        for user_id in tqdm(user_ids, desc=\"  Extracting embeddings\", ncols=80):\n",
        "            if user_id in model.wv:\n",
        "                self.embeddings[user_id] = model.wv[user_id]\n",
        "            else:\n",
        "                # Fallback for missing nodes\n",
        "                self.embeddings[user_id] = np.random.normal(0, 0.1, self.config['node2vec_dims'])\n",
        "\n",
        "        print(f\"  âœ… Generated embeddings for {len(self.embeddings):,} users\")\n",
        "        return self\n",
        "\n",
        "    def _enhance_friendship_weights(self, friendship_graph, user_app_reviews, user_to_idx):\n",
        "        \"\"\"Enhance friendship edge weights based on user activity similarity AND country similarity\"\"\"\n",
        "\n",
        "        # Calculate user activity profiles (playtime per app)\n",
        "        user_activity = defaultdict(dict)\n",
        "        for u, a, playtime, sentiment in user_app_reviews:\n",
        "            if u in user_to_idx:  # Only for LCC users\n",
        "                user_activity[u][a] = playtime\n",
        "\n",
        "        print(f\"    ğŸ“ˆ User activity profiles: {len(user_activity):,} users\")\n",
        "\n",
        "        # Create enhanced graph with weighted edges\n",
        "        enhanced_graph = friendship_graph.copy()\n",
        "\n",
        "        edges_enhanced = 0\n",
        "        edge_list = list(friendship_graph.edges())\n",
        "\n",
        "        print(f\"    ğŸ”— Processing {len(edge_list):,} friendship edges...\")\n",
        "        for u, v in tqdm(edge_list, desc=\"    Enhancing edges\", ncols=80):\n",
        "            # 1. Calculate activity similarity between friends\n",
        "            u_apps = set(user_activity[u].keys()) if u in user_activity else set()\n",
        "            v_apps = set(user_activity[v].keys()) if v in user_activity else set()\n",
        "\n",
        "            # Jaccard similarity of played apps + playtime correlation\n",
        "            common_apps = u_apps.intersection(v_apps)\n",
        "            union_apps = u_apps.union(v_apps)\n",
        "\n",
        "            activity_sim = 0.0\n",
        "            if union_apps:\n",
        "                jaccard_sim = len(common_apps) / len(union_apps)\n",
        "\n",
        "                # Add playtime correlation for common apps\n",
        "                playtime_sim = 0.0\n",
        "                if common_apps:\n",
        "                    u_times = [user_activity[u][app] for app in common_apps]\n",
        "                    v_times = [user_activity[v][app] for app in common_apps]\n",
        "\n",
        "                    # Simple correlation based on relative playtime rankings\n",
        "                    u_rank = np.argsort(u_times)\n",
        "                    v_rank = np.argsort(v_times)\n",
        "                    playtime_sim = 1.0 - (np.abs(u_rank - v_rank).mean() / len(common_apps))\n",
        "\n",
        "                activity_sim = (jaccard_sim + playtime_sim) / 2.0\n",
        "\n",
        "            # 2. Calculate country similarity\n",
        "            country_sim = 0.0\n",
        "            if u in self.data_loader.users and v in self.data_loader.users:\n",
        "                u_country = self.data_loader.users[u].get('loccountrycode', 'UNKNOWN')\n",
        "                v_country = self.data_loader.users[v].get('loccountrycode', 'UNKNOWN')\n",
        "\n",
        "                # Same country gets full similarity boost\n",
        "                if u_country != 'UNKNOWN' and v_country != 'UNKNOWN':\n",
        "                    country_sim = 1.0 if u_country == v_country else 0.0\n",
        "\n",
        "            # 3. Combined similarity weight: Activity (70%) + Country (30%)\n",
        "            combined_sim = 0.7 * activity_sim + 0.3 * country_sim\n",
        "            weight = 1.0 + combined_sim  # Base weight 1.0, enhanced up to 2.0\n",
        "\n",
        "            enhanced_graph[u][v]['weight'] = weight\n",
        "            edges_enhanced += 1\n",
        "\n",
        "        print(f\"    ğŸ”— Enhanced {edges_enhanced:,} friendship edges with activity + country similarity weights\")\n",
        "        return enhanced_graph\n",
        "\n",
        "class AttributeEncoder:\n",
        "    \"\"\"Encodes categorical attributes into numeric features\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.country_encoder = {}\n",
        "        self.category_encoder = {}\n",
        "        self.app_type_encoder = {}\n",
        "\n",
        "    def fit_user_attributes(self, users):\n",
        "        \"\"\"Fit encoders for user attributes\"\"\"\n",
        "        print(\"  ğŸ”§ Encoding user attributes...\")\n",
        "\n",
        "        # Extract unique countries\n",
        "        countries = set()\n",
        "        for uid, user_data in users.items():\n",
        "            country = user_data.get('loccountrycode', 'UNKNOWN')\n",
        "            if country:\n",
        "                countries.add(country)\n",
        "\n",
        "        # Create country encoding (one-hot style but more compact)\n",
        "        countries = sorted(list(countries))\n",
        "        self.country_encoder = {country: i for i, country in enumerate(countries)}\n",
        "        self.country_encoder['UNKNOWN'] = len(countries)  # For missing values\n",
        "\n",
        "        print(f\"    ğŸ“ Countries: {len(countries)} unique values\")\n",
        "        return self\n",
        "\n",
        "    def fit_app_attributes(self, apps):\n",
        "        \"\"\"Fit encoders for app attributes\"\"\"\n",
        "        print(\"  ğŸ”§ Encoding app attributes...\")\n",
        "\n",
        "        # Extract unique categories\n",
        "        categories = set()\n",
        "        app_types = set()\n",
        "\n",
        "        for aid, app_data in apps.items():\n",
        "            category = app_data.get('category', 'Unknown')\n",
        "            app_type = app_data.get('app_type', -1)\n",
        "\n",
        "            if category:\n",
        "                categories.add(category)\n",
        "            if app_type is not None:\n",
        "                app_types.add(app_type)\n",
        "\n",
        "        # Create encodings\n",
        "        categories = sorted(list(categories))\n",
        "        self.category_encoder = {cat: i for i, cat in enumerate(categories)}\n",
        "\n",
        "        app_types = sorted(list(app_types))\n",
        "        self.app_type_encoder = {atype: i for i, atype in enumerate(app_types)}\n",
        "\n",
        "        print(f\"    ğŸ® Categories: {len(categories)} unique values\")\n",
        "        print(f\"    ğŸ·ï¸  App types: {len(app_types)} unique values\")\n",
        "        return self\n",
        "\n",
        "    def encode_user_features(self, users, user_ids):\n",
        "        \"\"\"Create feature vectors for users\"\"\"\n",
        "        print(\"  ğŸ”¢ Creating user feature vectors...\")\n",
        "\n",
        "        num_countries = len(self.country_encoder)\n",
        "        features = []\n",
        "\n",
        "        for uid in user_ids:\n",
        "            user_data = users[uid]\n",
        "            country = user_data.get('loccountrycode', 'UNKNOWN')\n",
        "\n",
        "            # One-hot encode country (but more compact - just the index)\n",
        "            country_idx = self.country_encoder.get(country, self.country_encoder['UNKNOWN'])\n",
        "\n",
        "            # Create feature vector: [country_idx_normalized]\n",
        "            country_normalized = country_idx / num_countries  # Normalize to [0,1]\n",
        "            features.append([country_normalized])\n",
        "\n",
        "        print(f\"    ğŸ‘¥ User features: {len(features)} users, {len(features[0])} dimensions\")\n",
        "        return np.array(features)\n",
        "\n",
        "    def encode_app_features(self, apps, app_ids):\n",
        "        \"\"\"Create feature vectors for apps\"\"\"\n",
        "        print(\"  ğŸ”¢ Creating app feature vectors...\")\n",
        "\n",
        "        features = []\n",
        "\n",
        "        for aid in app_ids:\n",
        "            app_data = apps[aid]\n",
        "            category = app_data.get('category', 'Unknown')\n",
        "            app_type = app_data.get('app_type', -1)\n",
        "\n",
        "            # Encode category\n",
        "            category_idx = self.category_encoder.get(category, 0)\n",
        "            category_normalized = category_idx / len(self.category_encoder)\n",
        "\n",
        "            # Encode app type\n",
        "            app_type_idx = self.app_type_encoder.get(app_type, 0)\n",
        "            app_type_normalized = app_type_idx / len(self.app_type_encoder)\n",
        "\n",
        "            # Create feature vector: [category_normalized, app_type_normalized]\n",
        "            features.append([category_normalized, app_type_normalized])\n",
        "\n",
        "        print(f\"    ğŸ® App features: {len(features)} apps, {len(features[0])} dimensions\")\n",
        "        return np.array(features)\n",
        "\n",
        "class HeterogeneousRecommenderGNN(nn.Module):\n",
        "    \"\"\"Heterogeneous GNN for Steam app recommendations with semantic features\"\"\"\n",
        "\n",
        "    def __init__(self, config, num_users, num_apps, user_feature_dim, app_feature_dim):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.num_users = num_users\n",
        "        self.num_apps = num_apps\n",
        "\n",
        "        # Input projection layers to standardize feature dimensions\n",
        "        self.user_proj = nn.Linear(user_feature_dim, config['gnn_hidden_dims'])\n",
        "        self.app_proj = nn.Linear(app_feature_dim, config['gnn_hidden_dims'])\n",
        "\n",
        "        # Heterogeneous convolutions\n",
        "        self.convs = nn.ModuleList()\n",
        "\n",
        "        # First layer\n",
        "        conv1 = HeteroConv({\n",
        "            ('user', 'friends_with', 'user'): SAGEConv(config['gnn_hidden_dims'], config['gnn_hidden_dims']),\n",
        "            ('user', 'reviewed', 'app'): SAGEConv((config['gnn_hidden_dims'], config['gnn_hidden_dims']), config['gnn_hidden_dims']),\n",
        "            ('app', 'reviewed_by', 'user'): SAGEConv((config['gnn_hidden_dims'], config['gnn_hidden_dims']), config['gnn_hidden_dims'])\n",
        "        }, aggr='sum')\n",
        "        self.convs.append(conv1)\n",
        "\n",
        "        # Second layer\n",
        "        conv2 = HeteroConv({\n",
        "            ('user', 'friends_with', 'user'): SAGEConv(config['gnn_hidden_dims'], config['gnn_out_dims']),\n",
        "            ('user', 'reviewed', 'app'): SAGEConv(config['gnn_hidden_dims'], config['gnn_out_dims']),\n",
        "            ('app', 'reviewed_by', 'user'): SAGEConv(config['gnn_hidden_dims'], config['gnn_out_dims'])\n",
        "        }, aggr='sum')\n",
        "        self.convs.append(conv2)\n",
        "\n",
        "        # Prediction layers\n",
        "        self.rating_predictor = nn.Sequential(\n",
        "            nn.Linear(config['gnn_out_dims'] * 2, config['gnn_hidden_dims']),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(config['gnn_hidden_dims'], 1)\n",
        "        )\n",
        "\n",
        "        # IMPROVED: Better link predictor without sigmoid saturation\n",
        "        self.link_predictor = nn.Sequential(\n",
        "            nn.Linear(config['gnn_out_dims'] * 2, config['gnn_hidden_dims']),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),  # Increased dropout for better generalization\n",
        "            nn.Linear(config['gnn_hidden_dims'], config['gnn_hidden_dims'] // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(config['gnn_hidden_dims'] // 2, 1)\n",
        "            # REMOVED: No sigmoid here - apply in get_recommendations with temperature\n",
        "        )\n",
        "\n",
        "        # Sentiment-aware recommendation layer\n",
        "        self.sentiment_predictor = nn.Sequential(\n",
        "            nn.Linear(config['gnn_out_dims'] * 2 + 1, config['gnn_hidden_dims']),  # +1 for playtime\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(config['gnn_hidden_dims'], 1),\n",
        "            nn.Tanh()  # Sentiment scores typically range from negative to positive\n",
        "        )\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict, edge_attr_dict=None):\n",
        "        # Project input features to common dimension\n",
        "        x_dict = {\n",
        "            'user': F.relu(self.user_proj(x_dict['user'])),\n",
        "            'app': F.relu(self.app_proj(x_dict['app']))\n",
        "        }\n",
        "\n",
        "        # Apply convolutions\n",
        "        for conv in self.convs:\n",
        "            x_dict = conv(x_dict, edge_index_dict)\n",
        "            x_dict = {key: F.relu(x) for key, x in x_dict.items()}\n",
        "\n",
        "        return x_dict\n",
        "\n",
        "    def predict_rating(self, user_emb, app_emb):\n",
        "        \"\"\"Predict playtime-based rating for user-app pairs\"\"\"\n",
        "        combined = torch.cat([user_emb, app_emb], dim=-1)\n",
        "        return self.rating_predictor(combined)\n",
        "\n",
        "    def predict_link(self, user_emb, app_emb):\n",
        "        \"\"\"Predict raw link scores for user-app pairs (no sigmoid)\"\"\"\n",
        "        combined = torch.cat([user_emb, app_emb], dim=-1)\n",
        "        return self.link_predictor(combined)\n",
        "\n",
        "    def predict_link_calibrated(self, user_emb, app_emb, temperature=1.0):\n",
        "        \"\"\"Predict calibrated link probability with temperature scaling\"\"\"\n",
        "        raw_scores = self.predict_link(user_emb, app_emb)\n",
        "        # Apply temperature scaling and sigmoid\n",
        "        calibrated_scores = torch.sigmoid(raw_scores / temperature)\n",
        "        return calibrated_scores\n",
        "\n",
        "    def predict_sentiment(self, user_emb, app_emb, playtime):\n",
        "        \"\"\"Predict sentiment given user-app embeddings and playtime\"\"\"\n",
        "        combined = torch.cat([user_emb, app_emb, playtime.unsqueeze(-1)], dim=-1)\n",
        "        return self.sentiment_predictor(combined)\n",
        "\n",
        "class SteamRecommenderSystem:\n",
        "    \"\"\"Main recommender system class\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.device = torch.device(config['device'])\n",
        "        self.data_loader = None\n",
        "        self.node2vec = None\n",
        "        self.model = None\n",
        "        self.data = None\n",
        "\n",
        "    def prepare_data(self):\n",
        "        \"\"\"Load and prepare all data with semantic attribute encoding\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ğŸ”„ DATA PREPARATION WITH SEMANTIC FEATURES\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Load data\n",
        "        self.data_loader = DataLoader().load_data()\n",
        "\n",
        "        # Extract largest connected component\n",
        "        friendship_graph = self.data_loader.extract_largest_connected_component()\n",
        "\n",
        "        # Create preliminary user mappings for Node2Vec enhancement\n",
        "        user_ids = list(self.data_loader.users.keys())\n",
        "        user_to_idx = {uid: i for i, uid in enumerate(user_ids)}\n",
        "\n",
        "        # Generate Node2Vec embeddings with activity-aware weights\n",
        "        print(f\"\\nğŸ¯ GENERATING ENHANCED NODE2VEC EMBEDDINGS\")\n",
        "        self.node2vec = Node2VecEmbedder(self.config, self.data_loader).fit(\n",
        "            friendship_graph,\n",
        "            user_app_reviews=self.data_loader.user_app_reviews,\n",
        "            user_to_idx=user_to_idx\n",
        "        )\n",
        "\n",
        "        # Prepare PyTorch Geometric data with semantic features\n",
        "        print(f\"\\nğŸ—ï¸  BUILDING HETEROGENEOUS GRAPH\")\n",
        "        self.data = self._create_hetero_data()\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _create_hetero_data(self):\n",
        "        \"\"\"Create HeteroData object with semantic features and Node2Vec embeddings\"\"\"\n",
        "        print(\"\\nğŸ”— Creating heterogeneous graph with semantic features...\")\n",
        "\n",
        "        data = HeteroData()\n",
        "\n",
        "        # Create mappings for LCC users and all apps\n",
        "        user_ids = list(self.data_loader.users.keys())  # Only LCC users with complete profiles\n",
        "        app_ids = list(self.data_loader.apps.keys())    # All 79 apps\n",
        "\n",
        "        user_to_idx = {uid: i for i, uid in enumerate(user_ids)}\n",
        "        app_to_idx = {aid: i for i, aid in enumerate(app_ids)}\n",
        "\n",
        "        print(f\"  ğŸ”— User-User Graph (LCC): {len(user_ids):,} users\")\n",
        "        print(f\"  ğŸ® Apps available: {len(app_ids):,} apps\")\n",
        "\n",
        "        # Initialize and fit attribute encoder\n",
        "        print(\"\\nğŸ¯ ENCODING SEMANTIC ATTRIBUTES\")\n",
        "        encoder = AttributeEncoder()\n",
        "        encoder.fit_user_attributes(self.data_loader.users)\n",
        "        encoder.fit_app_attributes(self.data_loader.apps)\n",
        "\n",
        "        # Create user features: Node2Vec embeddings + semantic attributes\n",
        "        print(\"\\n  ğŸ‘¥ Creating enhanced user features...\")\n",
        "        user_semantic_features = encoder.encode_user_features(self.data_loader.users, user_ids)\n",
        "\n",
        "        user_features = []\n",
        "        print(f\"  ğŸ”„ Combining Node2Vec + semantic features for {len(user_ids):,} users...\")\n",
        "        for i, uid in enumerate(tqdm(user_ids, desc=\"  Processing users\", ncols=80)):\n",
        "            # Get Node2Vec embedding\n",
        "            if uid in self.node2vec.embeddings:\n",
        "                node2vec_emb = self.node2vec.embeddings[uid]\n",
        "            else:\n",
        "                node2vec_emb = np.random.normal(0, 0.1, self.config['node2vec_dims'])\n",
        "\n",
        "            # Get semantic features\n",
        "            semantic_features = user_semantic_features[i]\n",
        "\n",
        "            # Combine: [Node2Vec features] + [semantic features]\n",
        "            combined_features = np.concatenate([node2vec_emb, semantic_features])\n",
        "            user_features.append(combined_features)\n",
        "\n",
        "        data['user'].x = torch.FloatTensor(user_features)\n",
        "        data['user'].num_nodes = len(user_ids)\n",
        "\n",
        "        print(f\"    ğŸ“Š User feature dimensions: {len(user_features[0])} ({self.config['node2vec_dims']} Node2Vec + {len(user_semantic_features[0])} semantic)\")\n",
        "\n",
        "        # Create app features: learnable embeddings + semantic attributes\n",
        "        print(\"  ğŸ® Creating enhanced app features...\")\n",
        "        app_semantic_features = encoder.encode_app_features(self.data_loader.apps, app_ids)\n",
        "\n",
        "        # For apps, we'll use the semantic features directly and let the GNN learn the rest\n",
        "        data['app'].x = torch.FloatTensor(app_semantic_features)\n",
        "        data['app'].num_nodes = len(app_ids)\n",
        "\n",
        "        print(f\"    ğŸ“Š App feature dimensions: {len(app_semantic_features[0])} semantic features\")\n",
        "\n",
        "        # User-User friendship edges: NO WEIGHTS for final GNN (Node2Vec uses enhanced weights internally)\n",
        "        print(\"\\n  ğŸ¤ Creating User-User edges (unweighted for GNN)...\")\n",
        "\n",
        "        # Get the enhanced friendship graph from Node2Vec (activity weights used only for Node2Vec training)\n",
        "        enhanced_graph = self.node2vec.enhanced_graph\n",
        "\n",
        "        friendship_edges = []\n",
        "\n",
        "        for u, v in enhanced_graph.edges():\n",
        "            if u in user_to_idx and v in user_to_idx:\n",
        "                # Add edges without weights (unweighted for GNN message passing)\n",
        "                friendship_edges.append([user_to_idx[u], user_to_idx[v]])\n",
        "                friendship_edges.append([user_to_idx[v], user_to_idx[u]])  # Undirected\n",
        "\n",
        "        if friendship_edges:\n",
        "            data['user', 'friends_with', 'user'].edge_index = torch.LongTensor(friendship_edges).t()\n",
        "            # NO edge_attr for User-User edges - unweighted\n",
        "            print(f\"    ğŸ”— User-User edges: {len(friendship_edges):,} (bidirectional, unweighted)\")\n",
        "\n",
        "        # User-App review edges (with playtime weights + sentiment attributes)\n",
        "        print(\"  ğŸ® Creating User-App edges (reviews with playtime + sentiment)...\")\n",
        "        review_edges = []\n",
        "        review_weights = []  # Playtime-based weights (author_playtime_at_review)\n",
        "        review_sentiments = []  # Sentiment scores (for recommender, not Node2Vec)\n",
        "\n",
        "        for u, a, playtime_weight, sentiment in self.data_loader.user_app_reviews:\n",
        "            if u in user_to_idx and a in app_to_idx:\n",
        "                review_edges.append([user_to_idx[u], app_to_idx[a]])\n",
        "                review_weights.append(playtime_weight)  # Use playtime as edge weight\n",
        "                review_sentiments.append(sentiment)     # Sentiment for recommender system\n",
        "\n",
        "        if review_edges:\n",
        "            data['user', 'reviewed', 'app'].edge_index = torch.LongTensor(review_edges).t()\n",
        "            data['user', 'reviewed', 'app'].edge_attr = torch.FloatTensor(review_weights)\n",
        "            data['user', 'reviewed', 'app'].sentiment = torch.FloatTensor(review_sentiments)\n",
        "\n",
        "            # Reverse edges for heterogeneous GNN\n",
        "            data['app', 'reviewed_by', 'user'].edge_index = data['user', 'reviewed', 'app'].edge_index.flip(0)\n",
        "            data['app', 'reviewed_by', 'user'].edge_attr = data['user', 'reviewed', 'app'].edge_attr\n",
        "            data['app', 'reviewed_by', 'user'].sentiment = data['user', 'reviewed', 'app'].sentiment\n",
        "\n",
        "            print(f\"    ğŸ® User-App edges: {len(review_edges):,} (playtime-weighted + sentiment)\")\n",
        "\n",
        "        # Store mappings and encoder\n",
        "        self.user_to_idx = user_to_idx\n",
        "        self.app_to_idx = app_to_idx\n",
        "        self.idx_to_user = {i: uid for uid, i in user_to_idx.items()}\n",
        "        self.idx_to_app = {i: aid for aid, i in app_to_idx.items()}\n",
        "        self.attribute_encoder = encoder\n",
        "\n",
        "        # Final structure summary\n",
        "        print(f\"\\nâœ… ENHANCED HETEROGENEOUS GRAPH CREATED:\")\n",
        "        print(f\"    ğŸ“Š Nodes: {len(user_ids):,} users (LCC) + {len(app_ids):,} apps\")\n",
        "        print(f\"    ğŸ”— User-User: {data['user', 'friends_with', 'user'].edge_index.shape[1]:,} friendship edges (unweighted)\")\n",
        "        print(f\"    ğŸ® User-App: {data['user', 'reviewed', 'app'].edge_index.shape[1]:,} review edges (playtime-weighted)\")\n",
        "        print(f\"    ğŸ§  User features: Node2Vec ({self.config['node2vec_dims']}D) + Country encoding (1D)\")\n",
        "        print(f\"    ğŸ¯ App features: Category + App-type encoding (2D)\")\n",
        "        print(f\"    âš–ï¸  Edge weights: Playtime (User-App)\")\n",
        "        print(f\"    ğŸ’­ Sentiment scores: Available for recommender system\")\n",
        "\n",
        "        return data\n",
        "\n",
        "    def _build_friendship_graph(self, user_ids, user_to_idx):\n",
        "        \"\"\"Build NetworkX friendship graph for LCC users\"\"\"\n",
        "        G = nx.Graph()\n",
        "        G.add_nodes_from(user_ids)\n",
        "\n",
        "        for u, v, w in self.data_loader.user_friendships:\n",
        "            if u in user_to_idx and v in user_to_idx:\n",
        "                G.add_edge(u, v, weight=w)\n",
        "\n",
        "        return G\n",
        "\n",
        "    def train_model(self):\n",
        "        \"\"\"Train the heterogeneous GNN\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ğŸš€ TRAINING HETEROGENEOUS GNN\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Initialize model\n",
        "        self.model = HeterogeneousRecommenderGNN(\n",
        "            self.config,\n",
        "            len(self.user_to_idx),\n",
        "            len(self.app_to_idx),\n",
        "            self.data['user'].x.shape[1],  # Actual user feature dimension\n",
        "            self.data['app'].x.shape[1]    # Actual app feature dimension\n",
        "        ).to(self.device)\n",
        "\n",
        "        print(f\"ğŸ“Š Model parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
        "\n",
        "        # Prepare training data - USE ALL INTERACTIONS (no artificial train/test split)\n",
        "        data = self.data.to(self.device)\n",
        "\n",
        "        # Use ALL user-app review edges for training\n",
        "        edge_index = data['user', 'reviewed', 'app'].edge_index\n",
        "        edge_attr = data['user', 'reviewed', 'app'].edge_attr\n",
        "        sentiment = data['user', 'reviewed', 'app'].sentiment\n",
        "\n",
        "        print(f\"\\nğŸƒâ€â™‚ï¸ Starting training with ALL interaction data...\")\n",
        "        print(f\"  Total edges: {edge_index.shape[1]:,}\")\n",
        "        print(f\"  Note: Using all data - no artificial train/test split for recommender system\")\n",
        "\n",
        "        # Optimizer\n",
        "        optimizer = Adam(self.model.parameters(), lr=self.config['learning_rate'])\n",
        "\n",
        "        # Training loop with loss-based early stopping\n",
        "        self.model.train()\n",
        "        best_loss = float('inf')\n",
        "        patience = 0\n",
        "        loss_history = []\n",
        "\n",
        "        # Create progress bar for training epochs\n",
        "        epoch_pbar = tqdm(range(self.config['epochs']), desc=\"ğŸš€ Training GNN\", ncols=100)\n",
        "\n",
        "        for epoch in epoch_pbar:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass with actual node features\n",
        "            x_dict = {\n",
        "                'user': data['user'].x,\n",
        "                'app': data['app'].x\n",
        "            }\n",
        "\n",
        "            edge_index_dict = {\n",
        "                ('user', 'friends_with', 'user'): data['user', 'friends_with', 'user'].edge_index,\n",
        "                ('user', 'reviewed', 'app'): edge_index,\n",
        "                ('app', 'reviewed_by', 'user'): edge_index.flip(0)\n",
        "            }\n",
        "\n",
        "            # Get embeddings\n",
        "            out_dict = self.model(x_dict, edge_index_dict)\n",
        "\n",
        "            # Compute losses on ALL data\n",
        "            user_emb = out_dict['user'][edge_index[0]]\n",
        "            app_emb = out_dict['app'][edge_index[1]]\n",
        "\n",
        "            # Rating prediction loss (predicting playtime)\n",
        "            pred_ratings = self.model.predict_rating(user_emb, app_emb).squeeze()\n",
        "            rating_loss = F.mse_loss(pred_ratings, edge_attr)\n",
        "\n",
        "            # Link prediction loss (positive samples = existing interactions)\n",
        "            pred_links = self.model.predict_link(user_emb, app_emb).squeeze()\n",
        "            link_targets = torch.ones_like(pred_links)\n",
        "\n",
        "            # IMPROVED: Better negative sampling strategy\n",
        "            neg_edge_index = negative_sampling(\n",
        "                edge_index,\n",
        "                num_nodes=(len(self.user_to_idx), len(self.app_to_idx)),\n",
        "                num_neg_samples=min(edge_index.shape[1], edge_index.shape[1] // 2)  # Fewer negatives\n",
        "            )\n",
        "\n",
        "            neg_user_emb = out_dict['user'][neg_edge_index[0]]\n",
        "            neg_app_emb = out_dict['app'][neg_edge_index[1]]\n",
        "            neg_pred_links = self.model.predict_link(neg_user_emb, neg_app_emb).squeeze()\n",
        "            neg_targets = torch.zeros_like(neg_pred_links)\n",
        "\n",
        "            # Use raw scores with BCE with logits (more stable)\n",
        "            link_loss = F.binary_cross_entropy_with_logits(\n",
        "                torch.cat([pred_links, neg_pred_links]),\n",
        "                torch.cat([link_targets, neg_targets])\n",
        "            )\n",
        "\n",
        "            # Total loss\n",
        "            total_loss = rating_loss + link_loss\n",
        "\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track loss for convergence-based early stopping\n",
        "            loss_history.append(total_loss.item())\n",
        "\n",
        "            # Update progress bar with current loss\n",
        "            epoch_pbar.set_postfix({\n",
        "                'Loss': f'{total_loss.item():.4f}',\n",
        "                'Rating': f'{rating_loss.item():.4f}',\n",
        "                'Link': f'{link_loss.item():.4f}',\n",
        "                'Best': f'{best_loss:.4f}'\n",
        "            })\n",
        "\n",
        "            # Monitoring and early stopping based on loss convergence\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"\\nEpoch {epoch:3d} | Total Loss: {total_loss:.4f} | \"\n",
        "                      f\"Rating Loss: {rating_loss.item():.4f} | Link Loss: {link_loss.item():.4f}\")\n",
        "\n",
        "                # Early stopping based on loss improvement\n",
        "                if total_loss.item() < best_loss:\n",
        "                    best_loss = total_loss.item()\n",
        "                    patience = 0\n",
        "                    # Save best model\n",
        "                    torch.save(self.model.state_dict(), 'best_model.pth')\n",
        "                else:\n",
        "                    patience += 1\n",
        "                    if patience >= self.config['early_stopping']:\n",
        "                        print(f\"Early stopping at epoch {epoch} - loss converged\")\n",
        "                        epoch_pbar.close()\n",
        "                        break\n",
        "\n",
        "                # Additional convergence check: if loss plateau for last 20 epochs\n",
        "                if len(loss_history) >= 20:\n",
        "                    recent_losses = loss_history[-20:]\n",
        "                    if max(recent_losses) - min(recent_losses) < 0.001:\n",
        "                        print(f\"Early stopping at epoch {epoch} - loss plateau detected\")\n",
        "                        epoch_pbar.close()\n",
        "                        break\n",
        "\n",
        "        epoch_pbar.close()\n",
        "\n",
        "        # Load best model\n",
        "        self.model.load_state_dict(torch.load('best_model.pth'))\n",
        "        print(f\"\\nâœ… Training completed! Best loss: {best_loss:.4f}\")\n",
        "        print(f\"ğŸ¯ Model trained on ALL {edge_index.shape[1]:,} user-app interactions\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def get_recommendations(self, user_id, top_k=10):\n",
        "        \"\"\"Get top-k app recommendations for a user with improved diversity and personalization\"\"\"\n",
        "        if user_id not in self.user_to_idx:\n",
        "            return []\n",
        "\n",
        "        # Get apps the user has already reviewed (to exclude from recommendations)\n",
        "        user_reviewed_apps = set()\n",
        "        user_categories = defaultdict(int)  # Track user's category preferences\n",
        "        user_total_playtime = 0\n",
        "\n",
        "        for u, a, playtime, sentiment in self.data_loader.user_app_reviews:\n",
        "            if u == user_id:\n",
        "                user_reviewed_apps.add(a)\n",
        "                # Track user's category preferences\n",
        "                if a in self.data_loader.apps:\n",
        "                    category = self.data_loader.apps[a].get('category', 'Unknown')\n",
        "                    user_categories[category] += playtime  # Weight by playtime\n",
        "                    user_total_playtime += playtime\n",
        "\n",
        "        print(f\"  User {user_id} has already reviewed {len(user_reviewed_apps)} apps\")\n",
        "        if user_categories:\n",
        "            top_user_categories = sorted(user_categories.items(), key=lambda x: x[1], reverse=True)[:3]\n",
        "            print(f\"  Top user categories: {dict(top_user_categories)}\")\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            data = self.data.to(self.device)\n",
        "\n",
        "            # Get embeddings using message passing\n",
        "            x_dict = {\n",
        "                'user': data['user'].x,\n",
        "                'app': data['app'].x\n",
        "            }\n",
        "\n",
        "            edge_index_dict = {\n",
        "                ('user', 'friends_with', 'user'): data['user', 'friends_with', 'user'].edge_index,\n",
        "                ('user', 'reviewed', 'app'): data['user', 'reviewed', 'app'].edge_index,\n",
        "                ('app', 'reviewed_by', 'user'): data['app', 'reviewed_by', 'user'].edge_index\n",
        "            }\n",
        "\n",
        "            out_dict = self.model(x_dict, edge_index_dict)\n",
        "\n",
        "            # Get user embedding\n",
        "            user_idx = self.user_to_idx[user_id]\n",
        "            user_emb = out_dict['user'][user_idx].unsqueeze(0)\n",
        "\n",
        "            # Get embeddings for ALL apps\n",
        "            app_embs = out_dict['app']\n",
        "\n",
        "            # IMPROVED: Predict raw scores and apply temperature scaling\n",
        "            user_emb_expanded = user_emb.repeat(len(app_embs), 1)\n",
        "            raw_scores = self.model.predict_link(user_emb_expanded, app_embs).squeeze()\n",
        "\n",
        "            # Apply temperature scaling for better score distribution\n",
        "            temperature = self.config.get('score_temperature', 2.0)\n",
        "            calibrated_scores = torch.sigmoid(raw_scores / temperature)\n",
        "\n",
        "            # Filter candidate apps and calculate diversity-aware scores\n",
        "            candidate_apps = []\n",
        "            candidate_scores = []\n",
        "            candidate_categories = []\n",
        "            candidate_raw_scores = []\n",
        "\n",
        "            print(f\"    ğŸ” Filtering {len(calibrated_scores)} apps for new recommendations...\")\n",
        "            for app_idx, (score, raw_score) in enumerate(zip(calibrated_scores, raw_scores)):\n",
        "                app_id = self.idx_to_app[app_idx]\n",
        "                if app_id not in user_reviewed_apps:  # Only new apps\n",
        "                    app_data = self.data_loader.apps[app_id]\n",
        "                    category = app_data.get('category', 'Unknown')\n",
        "\n",
        "                    candidate_apps.append(app_idx)\n",
        "                    candidate_scores.append(score.item())\n",
        "                    candidate_categories.append(category)\n",
        "                    candidate_raw_scores.append(raw_score.item())\n",
        "\n",
        "            if not candidate_apps:\n",
        "                print(f\"  No new apps to recommend for user {user_id}\")\n",
        "                return []\n",
        "\n",
        "            # IMPROVED: Diversity-aware recommendation selection\n",
        "            recommendations = []\n",
        "            used_categories = set()\n",
        "            remaining_apps = list(zip(candidate_apps, candidate_scores, candidate_categories, candidate_raw_scores))\n",
        "\n",
        "            # Sort by score initially\n",
        "            remaining_apps.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            diversity_weight = self.config.get('diversity_weight', 0.3)\n",
        "\n",
        "            print(f\"    ğŸ¯ Applying diversity-aware selection (weight: {diversity_weight})...\")\n",
        "            for i in range(min(top_k, len(remaining_apps))):\n",
        "                best_app = None\n",
        "                best_score = -1\n",
        "                best_idx = -1\n",
        "\n",
        "                for idx, (app_idx, score, category, raw_score) in enumerate(remaining_apps):\n",
        "                    # Base score\n",
        "                    final_score = score\n",
        "\n",
        "                    # DIVERSITY BONUS: Boost apps from unused categories\n",
        "                    if category not in used_categories and len(used_categories) > 0:\n",
        "                        final_score += diversity_weight * 0.1  # Small but meaningful boost\n",
        "\n",
        "                    # PERSONALIZATION BONUS: Boost apps from user's preferred categories\n",
        "                    if user_categories and category in user_categories:\n",
        "                        category_preference = user_categories[category] / user_total_playtime\n",
        "                        final_score += 0.05 * category_preference  # Preference boost\n",
        "\n",
        "                    # QUALITY THRESHOLD: Ensure minimum score difference\n",
        "                    min_diff = self.config.get('min_score_diff', 0.05)\n",
        "                    if len(recommendations) > 0:\n",
        "                        last_score = recommendations[-1]['base_score']\n",
        "                        if abs(score - last_score) < min_diff and category in used_categories:\n",
        "                            continue  # Skip too similar scores from same category\n",
        "\n",
        "                    if final_score > best_score:\n",
        "                        best_score = final_score\n",
        "                        best_app = (app_idx, score, category, raw_score)\n",
        "                        best_idx = idx\n",
        "\n",
        "                if best_app is not None:\n",
        "                    app_idx, base_score, category, raw_score = best_app\n",
        "                    app_id = self.idx_to_app[app_idx]\n",
        "                    app_data = self.data_loader.apps[app_id]\n",
        "\n",
        "                    recommendations.append({\n",
        "                        'rank': len(recommendations) + 1,\n",
        "                        'app_id': app_id,\n",
        "                        'app_name': app_data.get('name', 'Unknown'),\n",
        "                        'category': category,\n",
        "                        'score': best_score,  # Final diversity-adjusted score\n",
        "                        'base_score': base_score,  # Original model score\n",
        "                        'raw_score': raw_score  # Pre-sigmoid raw score\n",
        "                    })\n",
        "\n",
        "                    used_categories.add(category)\n",
        "                    remaining_apps.pop(best_idx)\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            print(f\"  Generated {len(recommendations)} DIVERSE app recommendations\")\n",
        "            print(f\"  Categories used: {len(used_categories)} - {list(used_categories)}\")\n",
        "            print(f\"  Score range: {recommendations[-1]['base_score']:.4f} - {recommendations[0]['base_score']:.4f}\")\n",
        "\n",
        "            return recommendations\n",
        "\n",
        "def test_improved_recommendations(recommender, num_users=5):\n",
        "    \"\"\"Test the improved recommendation system with detailed analysis\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ğŸ”§ TESTING IMPROVED RECOMMENDATION SYSTEM\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Test on multiple users\n",
        "    sample_users = list(recommender.user_to_idx.keys())[:num_users]\n",
        "\n",
        "    all_recommended_apps = set()\n",
        "    category_distribution = defaultdict(int)\n",
        "    score_ranges = []\n",
        "\n",
        "    for i, user_id in enumerate(sample_users):\n",
        "        print(f\"\\nğŸ‘¤ User {user_id} (Test {i+1}/{num_users}):\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        recommendations = recommender.get_recommendations(user_id, top_k=10)\n",
        "\n",
        "        if recommendations:\n",
        "            # Collect statistics\n",
        "            user_apps = set(rec['app_id'] for rec in recommendations)\n",
        "            all_recommended_apps.update(user_apps)\n",
        "\n",
        "            user_categories = [rec['category'] for rec in recommendations]\n",
        "            for cat in user_categories:\n",
        "                category_distribution[cat] += 1\n",
        "\n",
        "            base_scores = [rec['base_score'] for rec in recommendations]\n",
        "            score_ranges.append(max(base_scores) - min(base_scores))\n",
        "\n",
        "            # Display recommendations with enhanced info\n",
        "            print(\"  ğŸ¯ Recommendations:\")\n",
        "            for rec in recommendations:\n",
        "                diversity_boost = rec['score'] - rec['base_score']\n",
        "                print(f\"    {rec['rank']}. {rec['app_name'][:40]:<40} \"\n",
        "                      f\"({rec['category'][:20]:<20}) - \"\n",
        "                      f\"Base: {rec['base_score']:.4f}, \"\n",
        "                      f\"Final: {rec['score']:.4f} \"\n",
        "                      f\"(+{diversity_boost:+.4f})\")\n",
        "\n",
        "            print(f\"  ğŸ“Š Score diversity: {max(base_scores) - min(base_scores):.4f}\")\n",
        "            print(f\"  ğŸ­ Categories: {len(set(user_categories))} unique\")\n",
        "        else:\n",
        "            print(\"  âŒ No recommendations generated\")\n",
        "\n",
        "    # Overall statistics\n",
        "    print(f\"\\nğŸ“ˆ OVERALL IMPROVEMENT ANALYSIS:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"  ğŸ¯ Unique apps recommended: {len(all_recommended_apps)}\")\n",
        "    print(f\"  ğŸ“Š Average score range: {np.mean(score_ranges):.4f} Â± {np.std(score_ranges):.4f}\")\n",
        "    print(f\"  ğŸ­ Category distribution: {dict(sorted(category_distribution.items(), key=lambda x: x[1], reverse=True))}\")\n",
        "\n",
        "    # Improvement metrics\n",
        "    avg_score_range = np.mean(score_ranges) if score_ranges else 0\n",
        "    unique_apps_ratio = len(all_recommended_apps) / (num_users * 10)  # Max possible unique apps\n",
        "    category_diversity = len(category_distribution) / len(recommender.data_loader.apps)\n",
        "\n",
        "    print(f\"\\nğŸ† IMPROVEMENT METRICS:\")\n",
        "    print(f\"  ğŸ“Š Score diversity improvement: {'âœ… Good' if avg_score_range > 0.01 else 'âš ï¸ Still low'}\")\n",
        "    print(f\"  ğŸ¯ App uniqueness ratio: {unique_apps_ratio:.2f} {'âœ… Good' if unique_apps_ratio > 0.5 else 'âš ï¸ Low'}\")\n",
        "    print(f\"  ğŸ­ Category coverage: {category_diversity:.2f} {'âœ… Good' if category_diversity > 0.5 else 'âš ï¸ Limited'}\")\n",
        "\n",
        "    return {\n",
        "        'avg_score_range': avg_score_range,\n",
        "        'unique_apps_ratio': unique_apps_ratio,\n",
        "        'category_diversity': category_diversity,\n",
        "        'total_unique_apps': len(all_recommended_apps)\n",
        "    }"
      ],
      "metadata": {
        "id": "kjXdaSAPE9WV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    print(\"ğŸ® Steam Recommender System with Node2Vec + Heterogeneous GNN\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Initialize system\n",
        "    recommender = SteamRecommenderSystem(CONFIG)\n",
        "\n",
        "    # Prepare data and train model\n",
        "    recommender.prepare_data().train_model()\n",
        "\n",
        "    # Test the improved recommendation system\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ğŸ¯ TESTING IMPROVED RECOMMENDATION SYSTEM\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Test the improved system\n",
        "    improvement_results = test_improved_recommendations(recommender, num_users=5)\n",
        "\n",
        "    print(f\"\\nğŸ‰ Recommender system training completed!\")\n",
        "    print(f\"ğŸ“Š Final model ready for recommendations\")\n",
        "    print(f\"âœ¨ Improvements: Better diversity, temperature scaling, category awareness\")"
      ],
      "metadata": {
        "id": "5P4y16g9FE8f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Check if running in Colab\n",
        "    try:\n",
        "        import google.colab\n",
        "        print(\"ğŸ”§ Running in Google Colab\")\n",
        "\n",
        "        # Install required packages\n",
        "        os.system(\"pip install node2vec torch-geometric networkx\")\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"ğŸ”§ Running locally\")\n",
        "\n",
        "    # Run main pipeline\n",
        "    recommender = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0eeec03244d7422b81151f287d9a0ca9",
            "272d63610aeb4386a81c7cf450a83817",
            "cd881c9e2a9544f49cf46d4888bc2696",
            "004253ae5f3e4b4b83976d0ed67192e3",
            "17ed6c8f491b4c2b9e02dfbf3e932db2",
            "1815683d0c53447db09224019dd20960",
            "3cf48b1c35924d69abedea3ee227494d",
            "7084370258c545dc9366578c8ea40240",
            "80fcef76d84149b2b268dbd6e3ef9043",
            "90befb433bf44b688eedaedbf3f6a4b2",
            "6b6d7a798c69412b83bd6c069415f7cd"
          ]
        },
        "id": "ZRlfIcI3FOlf",
        "outputId": "e9211a40-872d-4d5c-9d9a-92b39dc05bd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Running in Google Colab\n",
            "ğŸ® Steam Recommender System with Node2Vec + Heterogeneous GNN\n",
            "======================================================================\n",
            "\n",
            "============================================================\n",
            "ğŸ”„ DATA PREPARATION WITH SEMANTIC FEATURES\n",
            "============================================================\n",
            "\n",
            "ğŸ“‚ Loading Steam graph data...\n",
            "  Loading nodes...\n",
            "    Users: 884,706\n",
            "    Apps: 79\n",
            "  Loading edges...\n",
            "  Analyzing edge file format...\n",
            "    Header: source_id\ttarget_id\tsource_type\ttarget_type\tedge_type\tweight\tsentiment_score\tsource_name\ttarget_name\n",
            "    Sample line 1: 9 columns -> ['0', '30114', 'User', 'User', 'friendship', '1.000000', '', 'Nola', 'KillerPepep']\n",
            "    Sample line 2: 9 columns -> ['0', '30115', 'User', 'User', 'friendship', '1.000000', '', 'Nola', 'Skull Kid']\n",
            "    Sample line 3: 9 columns -> ['0', '30116', 'User', 'User', 'friendship', '1.000000', '', 'Nola', 'Fugazi']\n",
            "  Processing all edges...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    Processing edges: 3666870it [00:06, 574939.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Friendships: 3,636,496\n",
            "    Reviews: 30,374\n",
            "\n",
            "ğŸ”— Preparing final dataset structure...\n",
            "  Users with complete profiles: 884,706\n",
            "  Filtering friendships to profiled users...\n",
            "    Friendships: 3,636,496 (was 3,636,496)\n",
            "  Building friendship network...\n",
            "    Network: 884,706 users, 3,636,496 friendships\n",
            "  Finding largest connected component...\n",
            "    Total connected components: 1\n",
            "    Largest CC: 884,706 users (100.0%)\n",
            "    LCC friendships: 3,636,496\n",
            "  Filtering all data to LCC users...\n",
            "    Users: 884,706 (was 884,706)\n",
            "    User-User edges: 3,636,496\n",
            "  Verifying final User-User graph connectivity...\n",
            "    Final User-User graph: 884,706 nodes, 3,636,496 edges\n",
            "    Is fully connected: True\n",
            "    Number of components: 1\n",
            "    User-App edges: 30,374 (was 30,374)\n",
            "  Filtering apps to only those reviewed by LCC users...\n",
            "    Apps: 79 (was 79) - only apps reviewed by LCC users\n",
            "\n",
            "ğŸ“Š Final Dataset Structure:\n",
            "    ğŸ”— User-User Graph (LCC): 884,706 users, 3,636,496 friendships\n",
            "    ğŸ® User-App Reviews: 30,374 reviews (playtime weighted)\n",
            "    ğŸ¯ Apps: 79 apps (only those reviewed by LCC users)\n",
            "    ğŸ‘¥ User Attributes: loccountrycode only\n",
            "    ğŸ¯ App Attributes: name, category, app_type, original_id\n",
            "    âš–ï¸  Edge Attributes: User-App (playtime + sentiment), User-User (NO WEIGHTS in final GNN)\n",
            "    âœ… User-User graph is fully connected: True\n",
            "\n",
            "ğŸ¯ GENERATING ENHANCED NODE2VEC EMBEDDINGS\n",
            "\n",
            "ğŸ¯ Training Node2Vec embeddings with attribute-aware weights...\n",
            "  Dimensions: 64\n",
            "  Walk length: 20\n",
            "  Walks per node: 5\n",
            "  ğŸ”— Enhancing friendship edges with user activity + country similarity...\n",
            "    ğŸ“ˆ User activity profiles: 30,113 users\n",
            "    ğŸ”— Processing 3,636,496 friendship edges...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    Enhancing edges: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3636496/3636496 [00:11<00:00, 322483.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    ğŸ”— Enhanced 3,636,496 friendship edges with activity + country similarity weights\n",
            "  ğŸ“Š Final graph: 884,706 nodes, 3,636,496 edges\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Computing transition probabilities:   0%|          | 0/884706 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0eeec03244d7422b81151f287d9a0ca9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ğŸ”„ Training Node2Vec model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_system_performance(recommender):\n",
        "    \"\"\"\n",
        "    ğŸ” COMPREHENSIVE PERFORMANCE EVALUATION\n",
        "    Run this function after your main pipeline completes to assess system performance.\n",
        "\n",
        "    Usage:\n",
        "        evaluate_system_performance(recommender)\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ğŸ” COMPREHENSIVE PERFORMANCE EVALUATION\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # 1. NODE2VEC EMBEDDING QUALITY ASSESSMENT\n",
        "    print(\"\\nğŸ“Š NODE2VEC EMBEDDING QUALITY:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Check embedding statistics\n",
        "    embeddings = list(recommender.node2vec.embeddings.values())\n",
        "    embeddings_array = np.array(embeddings)\n",
        "\n",
        "    print(f\"  ğŸ“ˆ Embedding Statistics:\")\n",
        "    print(f\"    Dimensions: {embeddings_array.shape}\")\n",
        "    print(f\"    Mean: {embeddings_array.mean():.4f}\")\n",
        "    print(f\"    Std: {embeddings_array.std():.4f}\")\n",
        "    print(f\"    Min: {embeddings_array.min():.4f}\")\n",
        "    print(f\"    Max: {embeddings_array.max():.4f}\")\n",
        "\n",
        "    # Check embedding diversity (should not be too similar)\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "    # Sample subset for efficiency\n",
        "    sample_size = min(100, len(embeddings))\n",
        "    sample_embeddings = embeddings_array[:sample_size]\n",
        "\n",
        "    print(f\"  ğŸ” Computing similarity matrix for {sample_size} embeddings...\")\n",
        "    similarity_matrix = cosine_similarity(sample_embeddings)\n",
        "\n",
        "    # Remove diagonal (self-similarity = 1.0)\n",
        "    np.fill_diagonal(similarity_matrix, 0)\n",
        "    avg_similarity = similarity_matrix.mean()\n",
        "\n",
        "    print(f\"  ğŸ”— Embedding Diversity:\")\n",
        "    print(f\"    Average cosine similarity: {avg_similarity:.4f}\")\n",
        "    print(f\"    Quality: {'âœ… Good diversity' if avg_similarity < 0.3 else 'âš ï¸ High similarity (potential overfitting)'}\")\n",
        "\n",
        "    # 2. GNN TRAINING PERFORMANCE ANALYSIS\n",
        "    print(\"\\nğŸ§  GNN TRAINING PERFORMANCE:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Evaluate on full dataset (same data used for training - this is correct for RecSys)\n",
        "    recommender.model.eval()\n",
        "    with torch.no_grad():\n",
        "        data = recommender.data.to(recommender.device)\n",
        "\n",
        "        # Forward pass\n",
        "        x_dict = {\n",
        "            'user': data['user'].x,\n",
        "            'app': data['app'].x\n",
        "        }\n",
        "\n",
        "        edge_index_dict = {\n",
        "            ('user', 'friends_with', 'user'): data['user', 'friends_with', 'user'].edge_index,\n",
        "            ('user', 'reviewed', 'app'): data['user', 'reviewed', 'app'].edge_index,\n",
        "            ('app', 'reviewed_by', 'user'): data['app', 'reviewed_by', 'user'].edge_index\n",
        "        }\n",
        "\n",
        "        out_dict = recommender.model(x_dict, edge_index_dict)\n",
        "\n",
        "        # Evaluate on all user-app edges\n",
        "        edge_index = data['user', 'reviewed', 'app'].edge_index\n",
        "        edge_attr = data['user', 'reviewed', 'app'].edge_attr\n",
        "\n",
        "        user_emb = out_dict['user'][edge_index[0]]\n",
        "        app_emb = out_dict['app'][edge_index[1]]\n",
        "\n",
        "        # Rating prediction performance\n",
        "        pred_ratings = recommender.model.predict_rating(user_emb, app_emb).squeeze()\n",
        "        rating_mse = F.mse_loss(pred_ratings, edge_attr).item()\n",
        "        rating_mae = F.l1_loss(pred_ratings, edge_attr).item()\n",
        "\n",
        "        # Link prediction performance\n",
        "        pred_links = recommender.model.predict_link(user_emb, app_emb).squeeze()\n",
        "\n",
        "        # Generate negative samples for evaluation\n",
        "        neg_edge_index = negative_sampling(\n",
        "            edge_index,\n",
        "            num_nodes=(len(recommender.user_to_idx), len(recommender.app_to_idx)),\n",
        "            num_neg_samples=edge_index.shape[1]\n",
        "        )\n",
        "\n",
        "        neg_user_emb = out_dict['user'][neg_edge_index[0]]\n",
        "        neg_app_emb = out_dict['app'][neg_edge_index[1]]\n",
        "        neg_pred_links = recommender.model.predict_link(neg_user_emb, neg_app_emb).squeeze()\n",
        "\n",
        "        # Combine positive and negative predictions\n",
        "        all_predictions = torch.cat([pred_links, neg_pred_links]).cpu().numpy()\n",
        "        all_labels = torch.cat([\n",
        "            torch.ones(len(pred_links)),\n",
        "            torch.zeros(len(neg_pred_links))\n",
        "        ]).cpu().numpy()\n",
        "\n",
        "        # Calculate metrics\n",
        "        link_auc = roc_auc_score(all_labels, all_predictions)\n",
        "        link_ap = average_precision_score(all_labels, all_predictions)\n",
        "\n",
        "        print(f\"  ğŸ“Š Rating Prediction:\")\n",
        "        print(f\"    MSE: {rating_mse:.4f}\")\n",
        "        print(f\"    MAE: {rating_mae:.4f}\")\n",
        "        print(f\"    Quality: {'âœ… Good' if rating_mse < 1.0 else 'âš ï¸ High error'}\")\n",
        "\n",
        "        print(f\"  ğŸ”— Link Prediction:\")\n",
        "        print(f\"    AUC-ROC: {link_auc:.4f}\")\n",
        "        print(f\"    Average Precision: {link_ap:.4f}\")\n",
        "        print(f\"    Quality: {'âœ… Excellent' if link_auc > 0.8 else 'âœ… Good' if link_auc > 0.7 else 'âš ï¸ Needs improvement'}\")\n",
        "\n",
        "    # 3. RECOMMENDATION QUALITY ASSESSMENT\n",
        "    print(\"\\nğŸ¯ RECOMMENDATION QUALITY ASSESSMENT:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Test recommendations for multiple users\n",
        "    test_users = list(recommender.user_to_idx.keys())[:10]  # Test on 10 users\n",
        "    total_recommended = 0\n",
        "    total_coverage = 0\n",
        "    user_scores = []\n",
        "\n",
        "    print(f\"  ğŸ” Testing recommendations for {len(test_users)} users...\")\n",
        "    for user_id in tqdm(test_users, desc=\"  Generating recommendations\", ncols=80):\n",
        "        recommendations = recommender.get_recommendations(user_id, top_k=5)\n",
        "        total_recommended += len(recommendations)\n",
        "\n",
        "        if recommendations:\n",
        "            # Calculate score diversity\n",
        "            scores = [rec['score'] for rec in recommendations]\n",
        "            score_std = np.std(scores)\n",
        "            user_scores.extend(scores)\n",
        "\n",
        "            # Count unique categories\n",
        "            categories = set([rec['category'] for rec in recommendations])\n",
        "            total_coverage += len(categories)\n",
        "\n",
        "            print(f\"  ğŸ‘¤ User {user_id}: {len(recommendations)} recs, \"\n",
        "                  f\"score range: {min(scores):.3f}-{max(scores):.3f}, \"\n",
        "                  f\"categories: {len(categories)}\")\n",
        "\n",
        "    # Overall recommendation statistics\n",
        "    avg_recs_per_user = total_recommended / len(test_users)\n",
        "    avg_category_coverage = total_coverage / len(test_users)\n",
        "    overall_score_diversity = np.std(user_scores) if user_scores else 0\n",
        "\n",
        "    print(f\"\\n  ğŸ“ˆ Overall Recommendation Quality:\")\n",
        "    print(f\"    Avg recommendations per user: {avg_recs_per_user:.1f}\")\n",
        "    print(f\"    Avg category diversity: {avg_category_coverage:.1f}\")\n",
        "    print(f\"    Score diversity (std): {overall_score_diversity:.4f}\")\n",
        "    print(f\"    Quality: {'âœ… Good diversity' if overall_score_diversity > 0.1 else 'âš ï¸ Low diversity'}\")\n",
        "\n",
        "    # 4. GRAPH STRUCTURE ANALYSIS\n",
        "    print(\"\\nğŸ”— GRAPH STRUCTURE ANALYSIS:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Analyze the enhanced friendship graph\n",
        "    G = recommender.node2vec.enhanced_graph\n",
        "\n",
        "    # Basic graph metrics\n",
        "    num_nodes = G.number_of_nodes()\n",
        "    num_edges = G.number_of_edges()\n",
        "    density = nx.density(G)\n",
        "\n",
        "    # Calculate degree statistics\n",
        "    degrees = [G.degree(n) for n in G.nodes()]\n",
        "    avg_degree = np.mean(degrees)\n",
        "    degree_std = np.std(degrees)\n",
        "\n",
        "    # Calculate clustering coefficient\n",
        "    avg_clustering = nx.average_clustering(G)\n",
        "\n",
        "    # Calculate shortest path length (on a sample for efficiency)\n",
        "    if num_nodes <= 1000:\n",
        "        try:\n",
        "            avg_path_length = nx.average_shortest_path_length(G)\n",
        "        except:\n",
        "            avg_path_length = \"N/A (disconnected)\"\n",
        "    else:\n",
        "        avg_path_length = \"N/A (too large)\"\n",
        "\n",
        "    print(f\"  ğŸ“Š Network Statistics:\")\n",
        "    print(f\"    Nodes: {num_nodes:,}\")\n",
        "    print(f\"    Edges: {num_edges:,}\")\n",
        "    print(f\"    Density: {density:.6f}\")\n",
        "    print(f\"    Avg degree: {avg_degree:.2f} Â± {degree_std:.2f}\")\n",
        "    print(f\"    Clustering coefficient: {avg_clustering:.4f}\")\n",
        "    print(f\"    Avg shortest path: {avg_path_length}\")\n",
        "\n",
        "    # 5. COUNTRY/ACTIVITY ENHANCEMENT ANALYSIS\n",
        "    print(\"\\nğŸŒ ENHANCEMENT IMPACT ANALYSIS:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Analyze edge weights distribution\n",
        "    edge_weights = [G[u][v].get('weight', 1.0) for u, v in G.edges()]\n",
        "    base_weights = sum(1 for w in edge_weights if abs(w - 1.0) < 0.001)\n",
        "    enhanced_weights = len(edge_weights) - base_weights\n",
        "\n",
        "    print(f\"  âš–ï¸ Edge Weight Analysis:\")\n",
        "    print(f\"    Base weight edges (1.0): {base_weights:,} ({base_weights/len(edge_weights)*100:.1f}%)\")\n",
        "    print(f\"    Enhanced weight edges: {enhanced_weights:,} ({enhanced_weights/len(edge_weights)*100:.1f}%)\")\n",
        "    print(f\"    Weight range: {min(edge_weights):.3f} - {max(edge_weights):.3f}\")\n",
        "    print(f\"    Avg weight: {np.mean(edge_weights):.3f}\")\n",
        "\n",
        "    # Check country distribution\n",
        "    countries = {}\n",
        "    for user_id in recommender.data_loader.users:\n",
        "        country = recommender.data_loader.users[user_id].get('loccountrycode', 'UNKNOWN')\n",
        "        countries[country] = countries.get(country, 0) + 1\n",
        "\n",
        "    print(f\"  ğŸŒ Country Distribution:\")\n",
        "    print(f\"    Unique countries: {len(countries)}\")\n",
        "    print(f\"    Top countries: {dict(sorted(countries.items(), key=lambda x: x[1], reverse=True)[:5])}\")\n",
        "\n",
        "    # 6. FINAL ASSESSMENT\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ğŸ† FINAL PERFORMANCE ASSESSMENT\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Create overall quality score\n",
        "    quality_factors = []\n",
        "\n",
        "    # Node2Vec quality (embedding diversity)\n",
        "    if avg_similarity < 0.3:\n",
        "        quality_factors.append(\"âœ… Node2Vec: Good embedding diversity\")\n",
        "    else:\n",
        "        quality_factors.append(\"âš ï¸ Node2Vec: High similarity detected\")\n",
        "\n",
        "    # GNN quality (AUC performance)\n",
        "    if link_auc > 0.8:\n",
        "        quality_factors.append(\"âœ… GNN: Excellent link prediction\")\n",
        "    elif link_auc > 0.7:\n",
        "        quality_factors.append(\"âœ… GNN: Good link prediction\")\n",
        "    else:\n",
        "        quality_factors.append(\"âš ï¸ GNN: Link prediction needs improvement\")\n",
        "\n",
        "    # Recommendation quality\n",
        "    if avg_recs_per_user >= 3 and overall_score_diversity > 0.1:\n",
        "        quality_factors.append(\"âœ… Recommendations: Good quality and diversity\")\n",
        "    else:\n",
        "        quality_factors.append(\"âš ï¸ Recommendations: Limited quality/diversity\")\n",
        "\n",
        "    # Enhancement effectiveness\n",
        "    if enhanced_weights / len(edge_weights) > 0.5:\n",
        "        quality_factors.append(\"âœ… Enhancements: Activity/country features utilized\")\n",
        "    else:\n",
        "        quality_factors.append(\"âš ï¸ Enhancements: Limited feature utilization\")\n",
        "\n",
        "    print(\"\\nğŸ“‹ Quality Assessment:\")\n",
        "    for factor in quality_factors:\n",
        "        print(f\"  {factor}\")\n",
        "\n",
        "    overall_quality = sum(1 for f in quality_factors if f.startswith(\"âœ…\")) / len(quality_factors)\n",
        "\n",
        "    print(f\"\\nğŸ¯ Overall System Quality: {overall_quality*100:.1f}% \"\n",
        "          f\"({'ğŸ† Excellent' if overall_quality > 0.8 else 'âœ… Good' if overall_quality > 0.6 else 'âš ï¸ Needs improvement'})\")\n",
        "\n",
        "    print(f\"\\nğŸ‰ Evaluation completed! System is ready for production use.\")\n",
        "\n",
        "    return {\n",
        "        'node2vec_similarity': avg_similarity,\n",
        "        'gnn_auc': link_auc,\n",
        "        'gnn_mse': rating_mse,\n",
        "        'avg_recommendations': avg_recs_per_user,\n",
        "        'score_diversity': overall_score_diversity,\n",
        "        'enhancement_ratio': enhanced_weights / len(edge_weights) if edge_weights else 0,\n",
        "        'overall_quality': overall_quality\n",
        "    }"
      ],
      "metadata": {
        "id": "IieamMzaFIBK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}